{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":112509,"databundleVersionId":14254895,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":1803569,"sourceType":"datasetVersion","datasetId":1071580},{"sourceId":653332,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":493548,"modelId":508969}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Imports","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport copy\nimport os\nimport sklearn\nimport gc\nimport time\nimport timm\nimport random\nimport safetensors\nfrom safetensors.torch import load_file\nfrom PIL import Image\nfrom torch.utils.data import DataLoader\nfrom sklearn.preprocessing import StandardScaler\nfrom torchvision import transforms\nfrom torchvision import models\nfrom collections import defaultdict\nfrom sklearn.model_selection import GroupKFold\nfrom torch.utils.data import random_split\nfrom scipy.stats import zscore\nif os.environ.get('KAGGLE_KERNEL_RUN_TYPE') != 'Batch':\n    !pip install -q ipdb\n    import ipdb","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T10:20:29.515446Z","iopub.execute_input":"2025-11-20T10:20:29.515727Z","iopub.status.idle":"2025-11-20T10:20:32.794827Z","shell.execute_reply.started":"2025-11-20T10:20:29.515708Z","shell.execute_reply":"2025-11-20T10:20:32.793975Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"SET_SEED=42\ndef set_seed(seed=SET_SEED):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T10:20:32.796389Z","iopub.execute_input":"2025-11-20T10:20:32.796624Z","iopub.status.idle":"2025-11-20T10:20:32.803326Z","shell.execute_reply.started":"2025-11-20T10:20:32.796601Z","shell.execute_reply":"2025-11-20T10:20:32.802522Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"<function __main__.set_seed(seed=42)>"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"# Hyperparameters\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nBATCH_SIZE = 8\nNUM_FT_EPOCHS = 20\nNUM_BB_EPOCHS = 12\nLEARNING_RATE = 0.0001\nWEIGHT_DECAY = 1e-7\nNUM_FOLDS = 3\nGIVEN_WEIGHTS = [0.1, 0.1, 0.1, 0.5, 0.2]\nTARGET_COLS = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\nBASE_MODEL='resnet50'\nIMAGE_SIZE=(384,384)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T10:20:32.804124Z","iopub.execute_input":"2025-11-20T10:20:32.804444Z","iopub.status.idle":"2025-11-20T10:20:32.819545Z","shell.execute_reply.started":"2025-11-20T10:20:32.804424Z","shell.execute_reply":"2025-11-20T10:20:32.818968Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"def print_result(train_loss, val_loss, epoch_start, epoch, num_epochs, val_r2):\n    epoch_time = time.time() - epoch_start\n    mins, secs = divmod(epoch_time, 60)\n    \n    print(f'Epoch {epoch+1}/{num_epochs} - '\n          f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f} | '\n          f'R²: {val_r2:.4f} | '\n          f'Time: {int(mins)}m {int(secs)}s')\n\nclass BiomassDataset(torch.utils.data.Dataset):\n    def __init__(self, df, base_path, transform=None):\n        self.df = df\n        self.base_path = base_path\n        self.transform = transform\n        self.target_cols = TARGET_COLS\n        self.is_training = all(col in df.columns for col in self.target_cols)\n    \n    def __len__(self):\n        return len(self.df) * 2\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx // 2] \n        img_path = os.path.join(self.base_path, row['image_path'])\n        \n        image = Image.open(img_path).convert('RGB')\n        \n        half = idx % 2\n        width, height = image.size\n        if half == 0:\n            image = image.crop((0, 0, width // 2, height))  # Left half\n        else:\n            image = image.crop((width // 2, 0, width, height))  # Right half\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        if self.is_training:\n            targets = row[self.target_cols].values.astype('float32')\n            \n            return image, torch.tensor(targets, dtype=torch.float32)\n        else:            \n            return image, row['image_path']\n\nclass ExtraDataset(torch.utils.data.Dataset):\n    def __init__(self, df, img_path, transform=None):\n        self.df = df.reset_index(drop=True)\n        self.img_path = img_path\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_file = os.path.join(self.img_path, row['image_file_name'])\n        image = Image.open(img_file).convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        target = torch.tensor(row['dry_total'], dtype=torch.float32)\n        return image, target","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T10:20:32.821109Z","iopub.execute_input":"2025-11-20T10:20:32.821316Z","iopub.status.idle":"2025-11-20T10:20:32.836649Z","shell.execute_reply.started":"2025-11-20T10:20:32.821295Z","shell.execute_reply":"2025-11-20T10:20:32.836108Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"class PreTrainModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        model = timm.create_model(BASE_MODEL, pretrained=False, num_classes=0)\n        \n        ckpt_path = \"/kaggle/input/m/voxoff/resnet50/pytorch/default/1/model.safetensors\"\n        # model.load_state_dict(load_file(ckpt_path))\n\n\n        loaded_state_dict = load_file(ckpt_path)\n\n        new_state_dict = {}\n        for k, v in loaded_state_dict.items():\n            # Assuming the actual ResNet backbone weights are nested under 'resnet.encoder.'\n            if k.startswith('resnet.encoder.'):\n                new_key = k[len('resnet.encoder.'):] # Strip the prefix\n                new_state_dict[new_key] = v\n\n        # Load the modified state dict, allowing for non-matching keys (strict=False)\n        model.load_state_dict(new_state_dict, strict=False)\n\n        \n        self.backbone = model\n        in_features = self.backbone.num_features\n        \n        self.regression_head = nn.Sequential(\n            nn.Linear(in_features, 256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, 64),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(64, 1)\n        )\n    \n    def forward(self, x):\n        features = self.backbone(x)\n        return self.regression_head(features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T10:20:32.837290Z","iopub.execute_input":"2025-11-20T10:20:32.837454Z","iopub.status.idle":"2025-11-20T10:20:32.860610Z","shell.execute_reply.started":"2025-11-20T10:20:32.837440Z","shell.execute_reply":"2025-11-20T10:20:32.859865Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"class FinetuneModel(nn.Module):\n    def __init__(self, pretrained_backbone):\n        super().__init__()\n        self.backbone = pretrained_backbone\n        in_features = self.backbone.num_features\n        \n        self.regression_head = nn.Sequential(\n            nn.Linear(in_features, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(64, 5) # 5 outputs for competition\n        )\n    \n    def forward(self, x):\n        features = self.backbone(x)\n        return self.regression_head(features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T10:20:32.861320Z","iopub.execute_input":"2025-11-20T10:20:32.861513Z","iopub.status.idle":"2025-11-20T10:20:32.881337Z","shell.execute_reply.started":"2025-11-20T10:20:32.861498Z","shell.execute_reply":"2025-11-20T10:20:32.880519Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"base = '/kaggle/input'\ntrain_csv = f'{base}/csiro-biomass/train.csv'\ntest_csv = f'{base}/csiro-biomass/test.csv'\nextra_csv = f'{base}/grassclover-dataset/biomass_data/train/biomass_train_data.csv'\nextra_img = f'{base}/grassclover-dataset/biomass_data/train/images'\nbase_path = f'{base}/csiro-biomass/'\nsubmission_path = f'{base}/csiro-biomass/sample_submission.csv'\n\ndataset_df = pd.read_csv(train_csv)\ntest_df = pd.read_csv(test_csv)\nextra_df = pd.read_csv(extra_csv, sep=';')\nextra_img_path = extra_img\nunique_test_images = test_df['image_path'].unique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T10:42:02.355763Z","iopub.execute_input":"2025-11-20T10:42:02.356077Z","iopub.status.idle":"2025-11-20T10:42:02.378485Z","shell.execute_reply.started":"2025-11-20T10:42:02.356054Z","shell.execute_reply":"2025-11-20T10:42:02.377861Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"dataset_df['Sampling_Date'] = pd.to_datetime(dataset_df['Sampling_Date'], format='mixed')  # adjust format if needed\ndataset_df = dataset_df.pivot(\n    index=['image_path','Sampling_Date'],\n    columns='target_name',\n    values='target'\n).reset_index()\ndataset_df['Month'] = dataset_df['Sampling_Date'].dt.month","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T10:42:03.712091Z","iopub.execute_input":"2025-11-20T10:42:03.712453Z","iopub.status.idle":"2025-11-20T10:42:03.735366Z","shell.execute_reply.started":"2025-11-20T10:42:03.712430Z","shell.execute_reply":"2025-11-20T10:42:03.734645Z"}},"outputs":[{"name":"stdout","text":"Dry_Clover_g: mean=6.65, std=12.12\nDry_Dead_g: mean=12.04, std=12.40\nDry_Green_g: mean=26.62, std=25.40\nDry_Total_g: mean=45.32, std=27.98\nGDM_g: mean=33.27, std=24.94\n","output_type":"stream"},{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"target_name              image_path Sampling_Date  Dry_Clover_g  Dry_Dead_g  \\\n0            train/ID1011485656.jpg    2015-09-04        0.0000     31.9984   \n1            train/ID1012260530.jpg    2015-04-01        0.0000      0.0000   \n2            train/ID1025234388.jpg    2015-09-01        6.0500      0.0000   \n3            train/ID1028611175.jpg    2015-05-18        0.0000     30.9703   \n4            train/ID1035947949.jpg    2015-09-11        0.4343     23.2239   \n\ntarget_name  Dry_Green_g  Dry_Total_g    GDM_g  Month  \n0                16.2751      48.2735  16.2750      9  \n1                 7.6000       7.6000   7.6000      4  \n2                 0.0000       6.0500   6.0500      9  \n3                24.2376      55.2079  24.2376      5  \n4                10.5261      34.1844  10.9605      9  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>target_name</th>\n      <th>image_path</th>\n      <th>Sampling_Date</th>\n      <th>Dry_Clover_g</th>\n      <th>Dry_Dead_g</th>\n      <th>Dry_Green_g</th>\n      <th>Dry_Total_g</th>\n      <th>GDM_g</th>\n      <th>Month</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train/ID1011485656.jpg</td>\n      <td>2015-09-04</td>\n      <td>0.0000</td>\n      <td>31.9984</td>\n      <td>16.2751</td>\n      <td>48.2735</td>\n      <td>16.2750</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train/ID1012260530.jpg</td>\n      <td>2015-04-01</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>7.6000</td>\n      <td>7.6000</td>\n      <td>7.6000</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train/ID1025234388.jpg</td>\n      <td>2015-09-01</td>\n      <td>6.0500</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>6.0500</td>\n      <td>6.0500</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train/ID1028611175.jpg</td>\n      <td>2015-05-18</td>\n      <td>0.0000</td>\n      <td>30.9703</td>\n      <td>24.2376</td>\n      <td>55.2079</td>\n      <td>24.2376</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train/ID1035947949.jpg</td>\n      <td>2015-09-11</td>\n      <td>0.4343</td>\n      <td>23.2239</td>\n      <td>10.5261</td>\n      <td>34.1844</td>\n      <td>10.9605</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":57},{"cell_type":"code","source":"# def denormalize_targets(normalized_targets):\n#     \"\"\"\n#     Convert normalized targets back to original scale.\n    \n#     Args:\n#         normalized_targets: Tensor of shape [batch_size, 5] or [5] with normalized values\n    \n#     Returns:\n#         Tensor of same shape with denormalized values\n#     \"\"\"\n#     if normalized_targets.dim() == 1:\n#         # Single sample: [5]\n#         means = TARGET_MEANS.to(normalized_targets.device)\n#         stds = TARGET_STDS.to(normalized_targets.device)\n#         return normalized_targets * stds + means\n#     else:\n#         # Batch: [batch_size, 5]\n#         means = TARGET_MEANS.to(normalized_targets.device).unsqueeze(0)  # [1, 5]\n#         stds = TARGET_STDS.to(normalized_targets.device).unsqueeze(0)  # [1, 5]\n#         return normalized_targets * stds + means\n\n\n# # Normalization\n# target_stats = {}\n# for col in TARGET_COLS:\n#     target_stats[col] = {\n#         'mean': dataset_df[col].mean(),\n#         'std': dataset_df[col].std() + 1e-8\n#     }\n#     print(f\"{col}: mean={target_stats[col]['mean']:.2f}, std={target_stats[col]['std']:.2f}\")\n\n# # Store for later denormalization\n# TARGET_MEANS = torch.tensor([target_stats[col]['mean'] for col in TARGET_COLS], dtype=torch.float32)\n# TARGET_STDS = torch.tensor([target_stats[col]['std'] for col in TARGET_COLS], dtype=torch.float32)\n\n# dataset_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Pytorch","metadata":{}},{"cell_type":"code","source":"train_transform = transforms.Compose([\n    transforms.Resize(IMAGE_SIZE),\n    # transforms.RandomHorizontalFlip(p=0.5),\n    # transforms.RandomVerticalFlip(p=0.5),\n    # transforms.RandomRotation(15),\n    # transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                       std=[0.229, 0.224, 0.225])\n])\n\nval_transform = transforms.Compose([\n    transforms.Resize(IMAGE_SIZE),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                       std=[0.229, 0.224, 0.225])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T10:20:32.938822Z","iopub.execute_input":"2025-11-20T10:20:32.939027Z","iopub.status.idle":"2025-11-20T10:20:32.945001Z","shell.execute_reply.started":"2025-11-20T10:20:32.939011Z","shell.execute_reply":"2025-11-20T10:20:32.944453Z"}},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":"### Support Functions","metadata":{}},{"cell_type":"code","source":"def forward_pass(images, targets, optimizer, model, validation=False):\n    images = images.to(device)\n    targets = targets.to(device)\n    \n    if not validation: \n        optimizer.zero_grad()\n    \n    outputs = model(images)\n    \n    loss = combined_biomass_loss(outputs, targets)\n    \n    if not validation:\n        loss.backward()\n        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n    \n    return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T10:20:32.947292Z","iopub.execute_input":"2025-11-20T10:20:32.947822Z","iopub.status.idle":"2025-11-20T10:20:32.961536Z","shell.execute_reply.started":"2025-11-20T10:20:32.947805Z","shell.execute_reply":"2025-11-20T10:20:32.960773Z"}},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":"### Train Model","metadata":{"execution":{"iopub.status.busy":"2025-11-14T12:22:03.521564Z","iopub.status.idle":"2025-11-14T12:22:03.522083Z","shell.execute_reply":"2025-11-14T12:22:03.521846Z","shell.execute_reply.started":"2025-11-14T12:22:03.521827Z"}}},{"cell_type":"code","source":"def combined_biomass_loss(biomass_pred, biomass_true):\n    weights = torch.tensor(GIVEN_WEIGHTS, device=biomass_pred.device)\n\n    smooth_l1 = nn.SmoothL1Loss(reduction='none')\n    mse = nn.MSELoss(reduction='none')\n    \n    smooth_l1_loss = smooth_l1(biomass_pred, biomass_true)\n    mse_loss = mse(biomass_pred, biomass_true)\n    \n    combined = 0.3 * smooth_l1_loss + 0.7 * mse_loss\n    weighted_loss = (combined * weights).mean()\n    \n    return weighted_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T10:20:32.962354Z","iopub.execute_input":"2025-11-20T10:20:32.962613Z","iopub.status.idle":"2025-11-20T10:20:32.980310Z","shell.execute_reply.started":"2025-11-20T10:20:32.962592Z","shell.execute_reply":"2025-11-20T10:20:32.979366Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"def weighted_r2_score(sum_target, total_samples, sum_target_sq, ss_res):\n    mean_target = sum_target / total_samples\n    ss_tot = sum_target_sq - total_samples * (mean_target ** 2)\n\n    r2_per_output = 1 - ss_res / (ss_tot + 1e-10)\n\n    weights = torch.tensor(GIVEN_WEIGHTS, device=device)\n    r2_weighted = (r2_per_output * weights).sum() / weights.sum()\n    return r2_weighted","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T10:20:32.981072Z","iopub.execute_input":"2025-11-20T10:20:32.981347Z","iopub.status.idle":"2025-11-20T10:20:33.000872Z","shell.execute_reply.started":"2025-11-20T10:20:32.981326Z","shell.execute_reply":"2025-11-20T10:20:33.000065Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"def pretrain_phase(extra_df, extra_img_path, num_epochs=NUM_FT_EPOCHS):\n    print(\"=\" * 50)\n    print(\"PHASE 1: PRE-TRAINING ON EXTRA DATASET\")\n    \n    # Create dataset\n    extra_dataset = ExtraDataset(extra_df, extra_img_path, transform=train_transform)\n\n    generator = torch.Generator().manual_seed(SET_SEED)\n    \n    extra_train_size = int(0.8 * len(extra_dataset))\n    extra_dev_size = len(extra_dataset) - extra_train_size\n\n    extra_train_dataset, extra_dev_dataset = random_split(extra_dataset, [extra_train_size, extra_dev_size], generator=generator)\n\n    extra_loader = DataLoader(extra_train_dataset, batch_size=16, shuffle=True)\n    extra_dev_loader = DataLoader(extra_dev_dataset, batch_size=16, shuffle=False)  # usually no shuffle for dev\n    \n    # Initialize model\n    model = PreTrainModel().to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n    criterion = nn.SmoothL1Loss()\n    \n    gc.collect()\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n        for images, targets in extra_loader:\n            images, targets = images.to(device), targets.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(images).squeeze()\n            loss = criterion(outputs, targets)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            \n            total_loss += loss.item()\n            # validate_epoch(model, extra_dev_loader, criterion=criterion)\n        \n        print(f\"Pre-train Epoch {epoch+1}/{num_epochs} - Loss: {total_loss/len(extra_loader):.4f}\")\n    \n    return model.backbone ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T10:20:33.001799Z","iopub.execute_input":"2025-11-20T10:20:33.002013Z","iopub.status.idle":"2025-11-20T10:20:33.017904Z","shell.execute_reply.started":"2025-11-20T10:20:33.001996Z","shell.execute_reply":"2025-11-20T10:20:33.017327Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"def create_data_loaders(train_df, val_df, batch_size=BATCH_SIZE):\n    train_dataset = BiomassDataset(train_df, base_path, transform=train_transform)\n    val_dataset = BiomassDataset(val_df, base_path, transform=val_transform)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n    return train_loader, val_loader\n\ndef train_epoch(model, train_loader, optimizer):\n    model.train()\n    total_loss = 0\n    for images, targets in train_loader:\n        loss = forward_pass(images, targets, optimizer, model)\n        total_loss += loss\n    return total_loss / len(train_loader)\n\ndef validate_epoch(model, val_loader, criterion=None):\n    model.eval()\n    total_loss = 0\n    total_samples = 0\n\n    # Accumulate sums for R² calculation (no need to store all predictions)\n    ss_res = torch.zeros(5, device=device)\n    sum_target = torch.zeros(5, device=device)\n    sum_target_sq = torch.zeros(5, device=device)\n\n    with torch.no_grad():\n        for images, targets in val_loader:\n            images = images.to(device)\n            targets = targets.to(device)\n            \n            outputs = model(images)\n\n            if criterion:\n                loss = criterion(outputs, targets)\n            else:\n                loss = combined_biomass_loss(outputs, targets)\n            \n            total_loss += loss.item()\n            total_samples += outputs.shape[0] # batch_size\n            \n            ss_res += ((outputs - targets) ** 2).sum(dim=0)\n            sum_target += targets.sum(dim=0)\n            sum_target_sq += (targets ** 2).sum(dim=0)\n\n    r2_weighted = weighted_r2_score(sum_target, total_samples, sum_target_sq, ss_res)\n\n    return total_loss / len(val_loader), r2_weighted\n\ndef finetune_phase(pretrained_backbone, num_epochs=NUM_FT_EPOCHS):\n    print(\"\\n\" + \"=\" * 50)\n    print(\"PHASE 2: FINE-TUNING ON COMPETITION DATA\")\n    \n    r2 = []\n\n    for fold, (train_idx, val_idx) in enumerate(splits):\n        print(f'\\n--- Fold {fold + 1}/{NUM_FOLDS} ---')\n        \n        train_df = dataset_df.iloc[train_idx].copy()\n        val_df = dataset_df.iloc[val_idx].copy()\n        train_loader, val_loader = create_data_loaders(train_df, val_df)\n\n        model = FinetuneModel(copy.deepcopy(pretrained_backbone)).to(device)\n        for param in model.backbone.parameters():\n            param.requires_grad = False\n            \n        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n        \n        for epoch in range(num_epochs):\n            epoch_start = time.time()\n            train_loss = train_epoch(model, train_loader, optimizer)\n            val_loss, val_r2 = validate_epoch(model, val_loader)\n            \n            print_result(train_loss, val_loss, epoch_start, epoch, num_epochs, val_r2)\n\n        r2.append(val_r2.item())\n        fold_models.append(model)\n        print(f'Fold {fold + 1} complete!')\n\n    overall_r2 = np.array(r2).mean()\n    \n    print(f'\\nOverall R² across all folds: {overall_r2:.4f}')\n\n    return fold_models","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T10:20:33.018788Z","iopub.execute_input":"2025-11-20T10:20:33.019035Z","iopub.status.idle":"2025-11-20T10:20:33.045022Z","shell.execute_reply.started":"2025-11-20T10:20:33.019014Z","shell.execute_reply":"2025-11-20T10:20:33.044128Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"pretrained_backbone = pretrain_phase(extra_df, extra_img_path, num_epochs=NUM_BB_EPOCHS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T10:20:33.045861Z","iopub.execute_input":"2025-11-20T10:20:33.046075Z","iopub.status.idle":"2025-11-20T10:25:50.059782Z","shell.execute_reply.started":"2025-11-20T10:20:33.046058Z","shell.execute_reply":"2025-11-20T10:25:50.058919Z"}},"outputs":[{"name":"stdout","text":"==================================================\nPHASE 1: PRE-TRAINING ON EXTRA DATASET\nPre-train Epoch 1/12 - Loss: 65.4099\nPre-train Epoch 2/12 - Loss: 64.2731\nPre-train Epoch 3/12 - Loss: 62.7436\nPre-train Epoch 4/12 - Loss: 61.0524\nPre-train Epoch 5/12 - Loss: 58.8126\nPre-train Epoch 6/12 - Loss: 56.2219\nPre-train Epoch 7/12 - Loss: 53.2763\nPre-train Epoch 8/12 - Loss: 49.6969\nPre-train Epoch 9/12 - Loss: 45.2294\nPre-train Epoch 10/12 - Loss: 40.0301\nPre-train Epoch 11/12 - Loss: 34.7882\nPre-train Epoch 12/12 - Loss: 30.1635\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"kfold = GroupKFold(n_splits=NUM_FOLDS)\ngroups = dataset_df['Month']\nsplits = kfold.split(dataset_df, groups=groups)\nfold_models = []\nfinal_model = finetune_phase(pretrained_backbone, num_epochs=NUM_FT_EPOCHS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T10:25:50.061273Z","iopub.execute_input":"2025-11-20T10:25:50.061623Z","iopub.status.idle":"2025-11-20T10:40:31.524451Z","shell.execute_reply.started":"2025-11-20T10:25:50.061602Z","shell.execute_reply":"2025-11-20T10:40:31.523138Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nPHASE 2: FINE-TUNING ON COMPETITION DATA\n\n--- Fold 1/3 ---\nEpoch 1/20 - Train Loss: 250.2351, Val Loss: 237.0026 | R²: -1.7977 | Time: 0m 44s\nEpoch 2/20 - Train Loss: 173.9674, Val Loss: 108.6896 | R²: -0.4166 | Time: 0m 44s\nEpoch 3/20 - Train Loss: 92.2561, Val Loss: 61.4008 | R²: 0.1656 | Time: 0m 44s\nEpoch 4/20 - Train Loss: 80.9140, Val Loss: 75.0408 | R²: 0.0198 | Time: 0m 44s\nEpoch 5/20 - Train Loss: 73.6500, Val Loss: 78.8854 | R²: -0.0254 | Time: 0m 44s\nEpoch 6/20 - Train Loss: 70.5038, Val Loss: 90.1469 | R²: -0.1502 | Time: 0m 44s\nEpoch 7/20 - Train Loss: 70.0943, Val Loss: 87.7616 | R²: -0.1208 | Time: 0m 44s\nEpoch 8/20 - Train Loss: 61.0279, Val Loss: 108.9406 | R²: -0.3454 | Time: 0m 44s\nEpoch 9/20 - Train Loss: 64.8468, Val Loss: 88.0971 | R²: -0.0971 | Time: 0m 44s\nEpoch 10/20 - Train Loss: 60.8941, Val Loss: 81.8055 | R²: -0.0585 | Time: 0m 44s\nEpoch 11/20 - Train Loss: 57.7155, Val Loss: 81.0488 | R²: -0.0317 | Time: 0m 44s\nEpoch 12/20 - Train Loss: 54.0704, Val Loss: 80.8372 | R²: -0.0308 | Time: 0m 44s\nEpoch 13/20 - Train Loss: 50.3500, Val Loss: 90.1333 | R²: -0.1573 | Time: 0m 44s\nEpoch 14/20 - Train Loss: 54.8757, Val Loss: 87.7194 | R²: -0.0781 | Time: 0m 44s\nEpoch 15/20 - Train Loss: 49.2101, Val Loss: 95.2160 | R²: -0.1901 | Time: 0m 44s\nEpoch 16/20 - Train Loss: 45.5081, Val Loss: 80.3485 | R²: -0.0447 | Time: 0m 45s\nEpoch 17/20 - Train Loss: 42.1001, Val Loss: 68.8307 | R²: 0.0863 | Time: 0m 44s\nEpoch 18/20 - Train Loss: 43.4122, Val Loss: 76.7627 | R²: 0.0137 | Time: 0m 44s\nEpoch 19/20 - Train Loss: 37.4510, Val Loss: 70.6950 | R²: 0.0672 | Time: 0m 44s\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/824990541.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msplits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkfold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfold_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfinal_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinetune_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_backbone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_FT_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_48/790551798.py\u001b[0m in \u001b[0;36mfinetune_phase\u001b[0;34m(pretrained_backbone, num_epochs)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mepoch_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_r2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mprint_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_r2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_48/790551798.py\u001b[0m in \u001b[0;36mvalidate_epoch\u001b[0;34m(model, val_loader, criterion)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_48/642591601.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mhalf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    984\u001b[0m             \u001b[0mdeprecate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0mhas_transparency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"transparency\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":54},{"cell_type":"code","source":"test_dataset = BiomassDataset(test_df, base_path, transform=val_transform)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n\nensemble_outputs = []\nwith torch.no_grad():\n    for image, img_path in test_loader:\n        image = image.to(device)\n        \n        # Get predictions from all models\n        all_outputs = []\n        for model in fold_models:\n            model.eval()\n            outputs = model(image)\n            print(outputs)\n            all_outputs.append(outputs)\n            ensemble_outputs.append(torch.stack(all_outputs).mean(dim=0))\n        \nensemble_outputs = torch.cat(ensemble_outputs, dim=0)\n2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T10:40:31.524980Z","iopub.status.idle":"2025-11-20T10:40:31.525211Z","shell.execute_reply.started":"2025-11-20T10:40:31.525103Z","shell.execute_reply":"2025-11-20T10:40:31.525113Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset = BiomassDataset(test_df, base_path, transform=val_transform)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n\n# Dictionary to store predictions grouped by image_path\npreds_by_image = defaultdict(list)\n\nwith torch.no_grad():\n    for image, img_paths in test_loader:\n        image = image.to(device)\n        \n        # Get ensemble predictions from all models\n        all_outputs = []\n        for model in fold_models:\n            model.eval()\n            outputs = model(image)\n            all_outputs.append(outputs)\n        \n        # Average across models (ensemble)\n        ensemble_batch = torch.stack(all_outputs).mean(dim=0)  # [batch_size, 5]\n        \n        # Group predictions by image_path\n        # Since BiomassDataset returns 2 items per image (left/right halves),\n        # img_paths is a tuple/list of image paths (may have duplicates)\n        # We group by image_path and will average left/right later\n        batch_size = ensemble_batch.shape[0]\n        for i in range(batch_size):\n            img_path = img_paths[i]  # Get the image_path for this batch item\n            preds_by_image[img_path].append(ensemble_batch[i].cpu())\n\n# Average left/right predictions for each image\nimage_predictions = {}\nfor img_path, preds in preds_by_image.items():\n    # Stack and average: [num_halves, 5] -> [5]\n    # Each image should have exactly 2 predictions (left + right)\n    stacked_preds = torch.stack(preds)  # [2, 5] for left and right halves\n    image_predictions[img_path] = stacked_preds.mean(dim=0)  # Average to [5]ee\n\n# Check predictions for first image\nif len(image_predictions) > 0:\n    first_img_path = list(image_predictions.keys())[0]\n    print(f\"Image: {first_img_path}\")\n    print(f\"Predictions: {image_predictions[first_img_path].tolist()}\")\n    print(f\"Target columns: {TARGET_COLS}\")\nelse:\n    print(\"No predictions available yet. Run the test prediction cell first.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ensemble_outputs[0].tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T10:40:31.526178Z","iopub.status.idle":"2025-11-20T10:40:31.526444Z","shell.execute_reply.started":"2025-11-20T10:40:31.526320Z","shell.execute_reply":"2025-11-20T10:40:31.526333Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_df = pd.read_csv(submission_path)\n\nfor i, row in submission_df.iterrows():\n    img_path = test_df[test_df['sample_id'] == row['sample_id']]['image_path'].values[0]\n    target_name = test_df[test_df['sample_id'] == row['sample_id']]['target_name'].values[0]\n    \n    preds = ensemble_outputs\n    target_idx = TARGET_COLS.index(target_name)\n    submission_df.at[i, 'target'] = ensemble_outputs[i, target_idx].item()\n\nsubmission_df.to_csv('submission.csv', index=False)\nsubmission_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T10:40:31.527037Z","iopub.status.idle":"2025-11-20T10:40:31.527292Z","shell.execute_reply.started":"2025-11-20T10:40:31.527162Z","shell.execute_reply":"2025-11-20T10:40:31.527171Z"}},"outputs":[],"execution_count":null}]}