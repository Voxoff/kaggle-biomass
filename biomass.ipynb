{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-14T15:18:01.401608Z",
     "iopub.status.busy": "2025-11-14T15:18:01.401235Z",
     "iopub.status.idle": "2025-11-14T15:18:03.096292Z",
     "shell.execute_reply": "2025-11-14T15:18:03.095804Z",
     "shell.execute_reply.started": "2025-11-14T15:18:01.401572Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sklearn\n",
    "# import fastai\n",
    "# from fastai.vision.all import *\n",
    "import time\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "# Load a dataset into a Pandas Dataframe\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T15:18:03.096825Z",
     "iopub.status.busy": "2025-11-14T15:18:03.096683Z",
     "iopub.status.idle": "2025-11-14T15:18:03.104180Z",
     "shell.execute_reply": "2025-11-14T15:18:03.103789Z",
     "shell.execute_reply.started": "2025-11-14T15:18:03.096815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full train dataset shape is (1785, 9)\n"
     ]
    }
   ],
   "source": [
    "kaggle_path = './kaggle/input/csiro-biomass/train.csv'\n",
    "local_path = '/Users/guytabennett-jones/code/me/ai/kaggle/biomass/input/train.csv'\n",
    "dataset_df = pd.read_csv(local_path)\n",
    "print(\"Full train dataset shape is {}\".format(dataset_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T15:18:03.104519Z",
     "iopub.status.busy": "2025-11-14T15:18:03.104449Z",
     "iopub.status.idle": "2025-11-14T15:18:03.175131Z",
     "shell.execute_reply": "2025-11-14T15:18:03.174626Z",
     "shell.execute_reply.started": "2025-11-14T15:18:03.104511Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sampling_Date</th>\n",
       "      <th>Species</th>\n",
       "      <th>image_path</th>\n",
       "      <th>Pre_GSHH_NDVI</th>\n",
       "      <th>Height_Ave_cm</th>\n",
       "      <th>State</th>\n",
       "      <th>Dry_Clover_g</th>\n",
       "      <th>Dry_Dead_g</th>\n",
       "      <th>Dry_Green_g</th>\n",
       "      <th>Dry_Total_g</th>\n",
       "      <th>GDM_g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015/9/4</td>\n",
       "      <td>Ryegrass_Clover</td>\n",
       "      <td>train/ID1011485656.jpg</td>\n",
       "      <td>0.62</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>Tas</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>31.9984</td>\n",
       "      <td>16.2751</td>\n",
       "      <td>48.2735</td>\n",
       "      <td>16.2750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015/4/1</td>\n",
       "      <td>Lucerne</td>\n",
       "      <td>train/ID1012260530.jpg</td>\n",
       "      <td>0.55</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>NSW</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7.6000</td>\n",
       "      <td>7.6000</td>\n",
       "      <td>7.6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015/9/1</td>\n",
       "      <td>SubcloverDalkeith</td>\n",
       "      <td>train/ID1025234388.jpg</td>\n",
       "      <td>0.38</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>WA</td>\n",
       "      <td>6.0500</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.0500</td>\n",
       "      <td>6.0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015/5/18</td>\n",
       "      <td>Ryegrass</td>\n",
       "      <td>train/ID1028611175.jpg</td>\n",
       "      <td>0.66</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>Tas</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>30.9703</td>\n",
       "      <td>24.2376</td>\n",
       "      <td>55.2079</td>\n",
       "      <td>24.2376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015/9/11</td>\n",
       "      <td>Ryegrass</td>\n",
       "      <td>train/ID1035947949.jpg</td>\n",
       "      <td>0.54</td>\n",
       "      <td>3.5000</td>\n",
       "      <td>Tas</td>\n",
       "      <td>0.4343</td>\n",
       "      <td>23.2239</td>\n",
       "      <td>10.5261</td>\n",
       "      <td>34.1844</td>\n",
       "      <td>10.9605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sampling_Date            Species              image_path  Pre_GSHH_NDVI  \\\n",
       "0      2015/9/4    Ryegrass_Clover  train/ID1011485656.jpg           0.62   \n",
       "1      2015/4/1            Lucerne  train/ID1012260530.jpg           0.55   \n",
       "2      2015/9/1  SubcloverDalkeith  train/ID1025234388.jpg           0.38   \n",
       "3     2015/5/18           Ryegrass  train/ID1028611175.jpg           0.66   \n",
       "4     2015/9/11           Ryegrass  train/ID1035947949.jpg           0.54   \n",
       "\n",
       "   Height_Ave_cm State  Dry_Clover_g  Dry_Dead_g  Dry_Green_g  Dry_Total_g  \\\n",
       "0         4.6667   Tas        0.0000     31.9984      16.2751      48.2735   \n",
       "1        16.0000   NSW        0.0000      0.0000       7.6000       7.6000   \n",
       "2         1.0000    WA        6.0500      0.0000       0.0000       6.0500   \n",
       "3         5.0000   Tas        0.0000     30.9703      24.2376      55.2079   \n",
       "4         3.5000   Tas        0.4343     23.2239      10.5261      34.1844   \n",
       "\n",
       "     GDM_g  \n",
       "0  16.2750  \n",
       "1   7.6000  \n",
       "2   6.0500  \n",
       "3  24.2376  \n",
       "4  10.9605  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pivot = dataset_df.pivot(\n",
    "    index='image_path',\n",
    "    columns='target_name',\n",
    "    values='target'\n",
    ").reset_index()\n",
    "metadata_cols = [\n",
    "    'Sampling_Date','Species', 'image_path',\n",
    "    'Pre_GSHH_NDVI','Height_Ave_cm','State',\n",
    "]\n",
    "metadata = dataset_df[metadata_cols].drop_duplicates(subset='image_path')\n",
    "dataset_df = pd.merge(metadata, train_pivot, on='image_path')\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Data investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T15:18:03.175500Z",
     "iopub.status.busy": "2025-11-14T15:18:03.175417Z",
     "iopub.status.idle": "2025-11-14T15:18:03.177328Z",
     "shell.execute_reply": "2025-11-14T15:18:03.176954Z",
     "shell.execute_reply.started": "2025-11-14T15:18:03.175492Z"
    }
   },
   "outputs": [],
   "source": [
    "# df = dataset_df\n",
    "# df['ndvi_height'] = df['Pre_GSHH_NDVI'] * df['Height_Ave_cm']\n",
    "# # Check correlation between NDVI and each target type\n",
    "# for target_name in df['target_name'].unique():\n",
    "#     subset = df[df['target_name'] == target_name]\n",
    "#     corr = subset['ndvi_height'].corr(subset['target'])\n",
    "#     print(f\"{target_name}: {corr:.3f}\")\n",
    "\n",
    "# # Visualize NDVI vs target for each type\n",
    "# fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# for i, target_name in enumerate(df['target_name'].unique()):\n",
    "#     subset = df[df['target_name'] == target_name]\n",
    "#     axes[i].scatter(subset['Height_Ave_cm'], subset['target'], alpha=0.5)\n",
    "#     axes[i].set_xlabel('NDVI')\n",
    "#     axes[i].set_ylabel('Biomass (g)')\n",
    "#     axes[i].set_title(target_name)\n",
    "    \n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T15:18:03.177675Z",
     "iopub.status.busy": "2025-11-14T15:18:03.177599Z",
     "iopub.status.idle": "2025-11-14T15:18:03.179870Z",
     "shell.execute_reply": "2025-11-14T15:18:03.179512Z",
     "shell.execute_reply.started": "2025-11-14T15:18:03.177667Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ryegrass_Clover', 'Lucerne', 'SubcloverDalkeith', 'Ryegrass',\n",
       "       'Phalaris_Clover', 'SubcloverLosa', 'Clover', 'Fescue_CrumbWeed',\n",
       "       'Phalaris_Ryegrass_Clover', 'Phalaris', 'WhiteClover', 'Fescue',\n",
       "       'Phalaris_BarleyGrass_SilverGrass_SpearGrass_Clover_Capeweed',\n",
       "       'Phalaris_Clover_Ryegrass_Barleygrass_Bromegrass', 'Mixed'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df.Species.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T15:18:03.180249Z",
     "iopub.status.busy": "2025-11-14T15:18:03.180123Z",
     "iopub.status.idle": "2025-11-14T15:18:03.181805Z",
     "shell.execute_reply": "2025-11-14T15:18:03.181448Z",
     "shell.execute_reply.started": "2025-11-14T15:18:03.180239Z"
    }
   },
   "outputs": [],
   "source": [
    "# Standardize: Calculate mean and std for each target column (save these!)\n",
    "# target_cols = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\n",
    "# target_mean = train_pivot[target_cols].mean().values\n",
    "# target_std = train_pivot[target_cols].std().values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T15:18:03.182103Z",
     "iopub.status.busy": "2025-11-14T15:18:03.182037Z",
     "iopub.status.idle": "2025-11-14T15:18:03.735812Z",
     "shell.execute_reply": "2025-11-14T15:18:03.735312Z",
     "shell.execute_reply.started": "2025-11-14T15:18:03.182097Z"
    }
   },
   "outputs": [],
   "source": [
    "local_path = '/Users/guytabennett-jones/code/me/ai/kaggle/biomass/input/'\n",
    "kaggle_path = '/kaggle/input/csiro-biomass/'\n",
    "class BiomassDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, base_path, transform=None):\n",
    "        self.df = df\n",
    "        self.base_path = base_path\n",
    "        self.transform = transform\n",
    "        self.target_cols = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = local_path + self.df.iloc[idx]['image_path']\n",
    "        targets = self.df.iloc[idx][self.target_cols].values.astype('float32')\n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "            metadata = {\n",
    "        'ndvi': torch.tensor([self.df.iloc[idx]['Pre_GSHH_NDVI']], dtype=torch.float32),\n",
    "        'height': torch.tensor([self.df.iloc[idx]['Height_Ave_cm']], dtype=torch.float32)\n",
    "    }\n",
    "        \n",
    "        return image, torch.tensor(targets, dtype=torch.float32), metadata\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                       std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                       std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "# Create dataset and dataloader\n",
    "# dataset = BiomassDataset(train_pivot, '/kaggle/input/csiro-biomass/', transform=transform)\n",
    "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T15:18:03.736248Z",
     "iopub.status.busy": "2025-11-14T15:18:03.736103Z",
     "iopub.status.idle": "2025-11-14T15:18:03.809408Z",
     "shell.execute_reply": "2025-11-14T15:18:03.808872Z",
     "shell.execute_reply.started": "2025-11-14T15:18:03.736239Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMultiTaskModel\u001b[39;00m(\u001b[43mnn\u001b[49m.Module):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_states=\u001b[32m4\u001b[39m):\n\u001b[32m      3\u001b[39m         \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n",
      "\u001b[31mNameError\u001b[39m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super().__init__()\n",
    "        # Load pretrained backbone\n",
    "        self.backbone = timm.create_model('tf_efficientnetv2_m', pretrained=True, num_classes=0)  # num_classes=0 removes head\n",
    "        backbone_features = self.backbone.num_features  # Get feature dimension\n",
    "        \n",
    "        # Main head for biomass (5 outputs)\n",
    "        self.biomass_head = nn.Linear(backbone_features, 5)\n",
    "        # Auxiliary heads\n",
    "        self.ndvi_head = nn.Linear(backbone_features, 1)\n",
    "        self.height_head = nn.Linear(backbone_features, 1)\n",
    "    \n",
    "    def forward(self, x, training=True):\n",
    "        features = self.backbone(x)\n",
    "        biomass = self.biomass_head(features)\n",
    "        \n",
    "        if training:\n",
    "            ndvi = self.ndvi_head(features)\n",
    "            height = self.height_head(features)\n",
    "            return biomass, ndvi, height\n",
    "        else:\n",
    "            return biomass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-14T15:18:03.809815Z",
     "iopub.status.idle": "2025-11-14T15:18:03.810060Z",
     "shell.execute_reply": "2025-11-14T15:18:03.809887Z",
     "shell.execute_reply.started": "2025-11-14T15:18:03.809881Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import timm\n",
    "from torchvision import models\n",
    "\n",
    "# Load pretrained ResNet50\n",
    "# model = models.resnet50(weights=True)\n",
    "# model = timm.tf_efficientnetv2_m(weights=True)\n",
    "model = MultiTaskModel(num_states=4).to(device)\n",
    "\n",
    "# Freeze the pretrained layers (transfer learning)\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # Replace the final layer for regression\n",
    "# # ResNet50's final layer is called 'fc' and has 2048 input features\n",
    "# model.fc = nn.Linear(model.fc.in_features, 4)  # Output 1 value (biomass)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0007)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-14T15:18:03.810436Z",
     "iopub.status.idle": "2025-11-14T15:18:03.810552Z",
     "shell.execute_reply": "2025-11-14T15:18:03.810491Z",
     "shell.execute_reply.started": "2025-11-14T15:18:03.810486Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataset_df['month'] = pd.to_datetime(dataset_df['Sampling_Date']).dt.month\n",
    "# dataset_df.drop(['Sampling_Date'], axis=1, inplace=True)\n",
    "\n",
    "# # One-hot encode State\n",
    "# dataset_df = pd.get_dummies(dataset_df, columns=['State'], prefix='state')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-14T15:18:03.811015Z",
     "iopub.status.idle": "2025-11-14T15:18:03.811162Z",
     "shell.execute_reply": "2025-11-14T15:18:03.811079Z",
     "shell.execute_reply.started": "2025-11-14T15:18:03.811074Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split sets\n",
    "train_df, val_df = sklearn.model_selection.train_test_split(dataset_df, train_size=0.8, random_state=42, shuffle=True)\n",
    "# Create datasets with different transforms\n",
    "kaggle_path = '/kaggle/input/csiro-biomass/'\n",
    "local_path =  '/Users/guytabennett-jones/code/me/ai/kaggle/biomass/input/'\n",
    "train_dataset = BiomassDataset(train_df, local_path, transform=train_transform)\n",
    "val_dataset = BiomassDataset(val_df, local_path, transform=val_transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-14T15:18:03.811600Z",
     "iopub.status.idle": "2025-11-14T15:18:03.811744Z",
     "shell.execute_reply": "2025-11-14T15:18:03.811646Z",
     "shell.execute_reply.started": "2025-11-14T15:18:03.811641Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(images, targets, metadata, validation=False):\n",
    "    images = images.to(device)\n",
    "    targets = targets.to(device)\n",
    "    metadata = {k: v.to(device) for k, v in metadata.items()}\n",
    "    \n",
    "    if not validation: \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    outputs = model(images, training=True)\n",
    "    biomass_pred, ndvi_pred, height_pred = outputs\n",
    "    \n",
    "    loss_biomass = criterion(biomass_pred, targets)\n",
    "    loss_ndvi = criterion(ndvi_pred, metadata['ndvi'])\n",
    "    loss_height = criterion(height_pred, metadata['height'])\n",
    "    \n",
    "    total_loss = 1.0 * loss_biomass + 0.3 * loss_ndvi + 0.3 * loss_height\n",
    "    \n",
    "    if not validation:\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return total_loss.item(), loss_biomass.item()\n",
    "\n",
    "# **CHANGED: print_result tracks both total and biomass losses**\n",
    "def print_result(train_loss, val_loss, train_biomass_loss, val_biomass_loss, \n",
    "                 train_loader, val_loader, epoch_start, epoch, num_epochs):\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "    train_biomass_loss /= len(train_loader)\n",
    "    val_biomass_loss /= len(val_loader)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    mins, secs = divmod(epoch_time, 60)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs} - '\n",
    "          f'Total Loss: Train={train_loss:.4f}, Val={val_loss:.4f} | '\n",
    "          f'Biomass Loss: Train={train_biomass_loss:.4f}, Val={val_biomass_loss:.4f} | '\n",
    "          f'Time: {int(mins)}m {int(secs)}s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-14T15:18:03.812047Z",
     "iopub.status.idle": "2025-11-14T15:18:03.812415Z",
     "shell.execute_reply": "2025-11-14T15:18:03.812100Z",
     "shell.execute_reply.started": "2025-11-14T15:18:03.812095Z"
    }
   },
   "outputs": [],
   "source": [
    "# def find_lr(model, train_loader, start_lr=1e-7, end_lr=10, num_iter=100):\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=start_lr)\n",
    "#     lr_mult = (end_lr / start_lr) ** (1/num_iter)\n",
    "#     lrs,losses = [],[]\n",
    "#     model.train()\n",
    "#     for i,(images,targets) in enumerate(train_loader):\n",
    "#         print('.')\n",
    "#         if i >= num_iter: break\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(images.to(device))\n",
    "#         loss = criterion(outputs, targets.to(device))\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         lrs.append(optimizer.param_groups[0]['lr'])\n",
    "#         losses.append(loss.item())\n",
    "#         for g in optimizer.param_groups: g['lr'] *= lr_mult\n",
    "#     return lrs,losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-14T15:18:03.812776Z",
     "iopub.status.idle": "2025-11-14T15:18:03.812894Z",
     "shell.execute_reply": "2025-11-14T15:18:03.812826Z",
     "shell.execute_reply.started": "2025-11-14T15:18:03.812822Z"
    }
   },
   "outputs": [],
   "source": [
    "# lrs, losses = find_lr(model, train_loader, start_lr=1e-7, end_lr=2, num_iter=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-14T15:18:03.813766Z",
     "iopub.status.idle": "2025-11-14T15:18:03.814196Z",
     "shell.execute_reply": "2025-11-14T15:18:03.813845Z",
     "shell.execute_reply.started": "2025-11-14T15:18:03.813841Z"
    }
   },
   "outputs": [],
   "source": [
    "# plt.plot(lrs, losses)\n",
    "# plt.xscale('log')\n",
    "# plt.xlabel('Learning Rate')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.grid(True)\n",
    "# lrs, losses\n",
    "# # print(f\"Total batches in train_loader: {len(train_loader)}\")\n",
    "# # print(f\"Batch size: {train_loader.batch_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-14T12:22:03.521564Z",
     "iopub.status.idle": "2025-11-14T12:22:03.522083Z",
     "shell.execute_reply": "2025-11-14T12:22:03.521846Z",
     "shell.execute_reply.started": "2025-11-14T12:22:03.521827Z"
    }
   },
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-14T15:18:03.814689Z",
     "iopub.status.idle": "2025-11-14T15:18:03.814785Z",
     "shell.execute_reply": "2025-11-14T15:18:03.814741Z",
     "shell.execute_reply.started": "2025-11-14T15:18:03.814736Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_loss(outputs, targets, metadata, weights={'biomass': 1.0, 'ndvi': 0.3, 'height': 0.3}):\n",
    "    biomass_pred, ndvi_pred, height_pred = outputs\n",
    "    biomass_true, ndvi_true, height_true = targets, metadata['ndvi'], metadata['height']\n",
    "    \n",
    "    loss_biomass = criterion(biomass_pred, biomass_true)\n",
    "    loss_ndvi = criterion(ndvi_pred, ndvi_true)\n",
    "    loss_height = criterion(height_pred, height_true)\n",
    "    \n",
    "    total_loss = (weights['biomass'] * loss_biomass + \n",
    "                  weights['ndvi'] * loss_ndvi + \n",
    "                  weights['height'] * loss_height)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-14T15:18:03.815078Z",
     "iopub.status.idle": "2025-11-14T15:18:03.815547Z",
     "shell.execute_reply": "2025-11-14T15:18:03.815135Z",
     "shell.execute_reply.started": "2025-11-14T15:18:03.815122Z"
    }
   },
   "outputs": [],
   "source": [
    "num_epochs = 25\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_biomass_loss = 0\n",
    "    \n",
    "    for images, targets, metadata in train_loader:\n",
    "        total_loss, biomass_loss = train_model(images, targets, metadata)\n",
    "        train_loss += total_loss\n",
    "        train_biomass_loss += biomass_loss\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_biomass_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, targets, metadata in val_loader:\n",
    "            total_loss, biomass_loss = train_model(images, targets, metadata, validation=True)\n",
    "            val_loss += total_loss\n",
    "            val_biomass_loss += biomass_loss\n",
    "    \n",
    "    print_result(train_loss, val_loss, train_biomass_loss, val_biomass_loss,\n",
    "                 train_loader, val_loader, epoch_start, epoch, num_epochs)\n",
    "    \n",
    "print(f'Training complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resnet50 322\n",
    "With aug Val Loss: 404.3574  \n",
    "Epoch 10 / 10  →  Train Loss: 404.0312   |   Val Loss: 526.2535  \n",
    "\n",
    "✅ Training complete!  \n",
    "Final batch loss: **730.4459**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-14T15:18:03.815839Z",
     "iopub.status.idle": "2025-11-14T15:18:03.816021Z",
     "shell.execute_reply": "2025-11-14T15:18:03.815887Z",
     "shell.execute_reply.started": "2025-11-14T15:18:03.815883Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the predictions for testdata\n",
    "with torch.no_grad():\n",
    "        for images, _ in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            preds.extend(outputs.cpu().numpy())\n",
    "predictions = np.array(preds).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-14T15:18:03.817031Z",
     "iopub.status.idle": "2025-11-14T15:18:03.817243Z",
     "shell.execute_reply": "2025-11-14T15:18:03.817085Z",
     "shell.execute_reply.started": "2025-11-14T15:18:03.817080Z"
    }
   },
   "outputs": [],
   "source": [
    "# Submit\n",
    "sample_submission_df = pd.read_csv('/kaggle/input/csiro-biomass/sample_submission.csv')\n",
    "sample_submission_df['target'] = predictions\n",
    "sample_submission_df.to_csv('/kaggle/working/submission.csv', index=False)\n",
    "sample_submission_df.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14254895,
     "sourceId": 112509,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3.13 (ai-venv)",
   "language": "python",
   "name": "ai-venv-3.13"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
