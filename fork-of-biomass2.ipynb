{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":112509,"databundleVersionId":14254895,"sourceType":"competition"},{"sourceId":1803569,"sourceType":"datasetVersion","datasetId":1071580},{"sourceId":653332,"sourceType":"modelInstanceVersion","modelInstanceId":493548,"modelId":508969}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Imports","metadata":{}},{"cell_type":"code","source":"import os\nSET_SEED=42\nos.environ['PYTHONHASHSEED'] = str(SET_SEED)\nos.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T23:29:58.411022Z","iopub.execute_input":"2025-11-25T23:29:58.411612Z","iopub.status.idle":"2025-11-25T23:29:58.415535Z","shell.execute_reply.started":"2025-11-25T23:29:58.411588Z","shell.execute_reply":"2025-11-25T23:29:58.414730Z"}},"outputs":[],"execution_count":140},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport copy\nimport sklearn\nimport gc\nimport timm\nimport time\nfrom torchvision.models import resnet50, ResNet50_Weights\nimport random\nimport safetensors\nfrom safetensors.torch import load_file\nfrom PIL import Image\nfrom torch.utils.data import DataLoader\nfrom sklearn.preprocessing import StandardScaler\nfrom torchvision import transforms\nfrom torchvision import models\nfrom collections import defaultdict\nfrom sklearn.model_selection import GroupKFold\nfrom torch.utils.data import random_split\nfrom scipy.stats import zscore\n# if os.environ.get('KAGGLE_KERNEL_RUN_TYPE') != 'Batch':\n#     !pip install -q ipdb\n#     import ipdb","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"execution":{"iopub.status.busy":"2025-11-25T23:29:58.421034Z","iopub.execute_input":"2025-11-25T23:29:58.421269Z","iopub.status.idle":"2025-11-25T23:29:58.430022Z","shell.execute_reply.started":"2025-11-25T23:29:58.421247Z","shell.execute_reply":"2025-11-25T23:29:58.429563Z"}},"outputs":[],"execution_count":141},{"cell_type":"code","source":"def set_seed(seed=SET_SEED):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    try:\n        torch.use_deterministic_algorithms(True, warn_only=True)\n    except AttributeError:\n        pass  # Older PyTorch versions\n    \n\nset_seed(SET_SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T23:29:58.433767Z","iopub.execute_input":"2025-11-25T23:29:58.433967Z","iopub.status.idle":"2025-11-25T23:29:58.447134Z","shell.execute_reply.started":"2025-11-25T23:29:58.433953Z","shell.execute_reply":"2025-11-25T23:29:58.446556Z"}},"outputs":[],"execution_count":142},{"cell_type":"code","source":"# Hyperparameters\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nBATCH_SIZE = 16\nNUM_FT_EPOCHS = 20\nNUM_BB_EPOCHS = 12\nLEARNING_RATE = 0.0001\nWEIGHT_DECAY = 1e-4\nNUM_FOLDS = 3\nGIVEN_WEIGHTS = [0.1, 0.1, 0.1, 0.5, 0.2]\nTARGET_COLS = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\nBASE_MODEL='efficientnet_b1'\nIMAGE_SIZE=(392,392)\nTRAIN_SHUFFLE=0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T23:29:58.448353Z","iopub.execute_input":"2025-11-25T23:29:58.448568Z","iopub.status.idle":"2025-11-25T23:29:58.458690Z","shell.execute_reply.started":"2025-11-25T23:29:58.448548Z","shell.execute_reply":"2025-11-25T23:29:58.457952Z"}},"outputs":[],"execution_count":143},{"cell_type":"code","source":"def print_result(train_loss, val_loss, epoch_start, epoch, num_epochs, val_r2):\n    epoch_time = time.time() - epoch_start\n    mins, secs = divmod(epoch_time, 60)\n    \n    print(f'Epoch {epoch+1}/{num_epochs} - '\n          f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f} | '\n          f'R²: {val_r2:.4f} | '\n          f'Time: {int(mins)}m {int(secs)}s')\n\nclass BiomassDataset(torch.utils.data.Dataset):\n    def __init__(self, df, base_path, transform=None):\n        self.df = df\n        self.base_path = base_path\n        self.transform = transform\n        self.target_cols = TARGET_COLS\n        self.is_training = all(col in df.columns for col in self.target_cols)\n    \n    def __len__(self):\n        return len(self.df) * 5\n\n    def _get_crop(self, image, crop_type):\n        \"\"\"Get different crop from image\"\"\"\n        width, height = image.size\n        \n        if crop_type == 0:  # Left half\n            return image.crop((0, 0, width // 2, height))\n        elif crop_type == 1:  # Right half\n            return image.crop((width // 2, 0, width, height))\n        elif crop_type == 2:  # Top half\n            return image.crop((0, 0, width, height // 2))\n        elif crop_type == 3:  # Bottom half\n            return image.crop((0, height // 2, width, height))\n        else:  # Center crop (80% of image)\n            crop_w, crop_h = int(width * 0.8), int(height * 0.8)\n            left = (width - crop_w) // 2\n            top = (height - crop_h) // 2\n            return image.crop((left, top, left + crop_w, top + crop_h))\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx // 5] \n        img_path = os.path.join(self.base_path, row['image_path'])\n        \n        image = Image.open(img_path).convert('RGB')\n        # image = self._get_crop(image, crop_type)\n        \n        # half = idx % 2\n        # width, height = image.size\n        # if half == 0:\n        #     image = image.crop((0, 0, width // 2, height))  # Left half\n        # else:\n        #     image = image.crop((width // 2, 0, width, height))  # Right half\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        if self.is_training:\n            targets = row[self.target_cols].values.astype('float32')\n            targets_normalized = (targets - TARGET_MEANS.numpy()) / TARGET_STDS.numpy()\n            \n            return image, torch.tensor(targets_normalized, dtype=torch.float32)\n        else:            \n            return image, row['image_path']\n\nclass ExtraDataset(torch.utils.data.Dataset):\n    def __init__(self, df, img_path, transform=None):\n        self.df = df.reset_index(drop=True)\n        self.img_path = img_path\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_file = os.path.join(self.img_path, row['image_file_name'])\n        image = Image.open(img_file).convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        target = torch.tensor(row['dry_total'], dtype=torch.float32)\n        return image, target","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T23:29:58.459452Z","iopub.execute_input":"2025-11-25T23:29:58.459676Z","iopub.status.idle":"2025-11-25T23:29:58.473599Z","shell.execute_reply.started":"2025-11-25T23:29:58.459652Z","shell.execute_reply":"2025-11-25T23:29:58.472996Z"}},"outputs":[],"execution_count":144},{"cell_type":"code","source":"class PreTrainModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # model = timm.create_model(BASE_MODEL, pretrained=True, num_classes=0)\n        \n        # ckpt_path = \"/kaggle/input/m/voxoff/resnet50/pytorch/default/1/model.safetensors\"\n        # # model.load_state_dict(load_file(ckpt_path))\n\n        # loaded_state_dict = load_file(ckpt_path)\n\n        # new_state_dict = {}\n        # for k, v in loaded_state_dict.items():\n        #     # Assuming the actual ResNet backbone weights are nested under 'resnet.encoder.'\n        #     if k.startswith('resnet.encoder.'):\n        #         new_key = k[len('resnet.encoder.'):] # Strip the prefix\n        #         new_state_dict[new_key] = v\n\n        # # Load the modified state dict, allowing for non-matching keys (strict=False)\n        # model.load_state_dict(new_state_dict, strict=False)\n\n        \n        # self.backbone = model\n        # in_features = self.backbone.num_features\n        \n        self.regression_head = nn.Sequential(\n            nn.Linear(in_features, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 64),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(64, 1)\n        )\n    \n    def forward(self, x):\n        features = self.backbone(x)\n        return self.regression_head(features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T23:29:58.474890Z","iopub.execute_input":"2025-11-25T23:29:58.475063Z","iopub.status.idle":"2025-11-25T23:29:58.487513Z","shell.execute_reply.started":"2025-11-25T23:29:58.475042Z","shell.execute_reply":"2025-11-25T23:29:58.486874Z"}},"outputs":[],"execution_count":145},{"cell_type":"code","source":"# from torchvision.models import resnet18, resnet34, resnet50, efficientnet_b0, efficientnet_b1\n\nclass FinetuneModel(nn.Module):\n    def __init__(self, pretrained_backbone=None):\n        super().__init__()\n        # self.backbone = timm.create_model('tf_efficientnet_b1', pretrained=False, num_classes = 0, checkpoint_path='')\n        self.backbone = timm.create_model('tf_efficientnet_b1', pretrained=False, num_classes=0)\n        model_path = '/kaggle/input/tf-efficientnet/pytorch/tf-efficientnet-b1/1/tf_efficientnet_b1_aa-ea7a6ee0.pth'\n        checkpoint = torch.load(model_path)\n        self.backbone.load_state_dict(checkpoint, strict=False)\n        \n        feature_dim = self.backbone.num_features\n        \n        # backbone_name=BASE_MODEL\n        # if backbone_name == \"efficientnet_b0\":\n        # self.backbone = efficientnet_b0(weights=\"IMAGENET1K_V1\")\n        # feature_dim = self.backbone.classifier[1].in_features\n        # self.backbone.classifier = nn.Identity()\n        # elif backbone_name == \"efficientnet_b1\":\n        #     self.backbone = efficientnet_b1(weights=\"IMAGENET1K_V1\")\n        #     feature_dim = self.backbone.classifier[1].in_features\n        #     self.backbone.classifier = nn.Identity()\n        # else:\n        #     raise ValueError(f\"Backbone {backbone_name} not supported.\")\n      \n\n        self.regression_head = nn.Sequential(\n            nn.Linear(feature_dim, 128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.Linear(64, 5), # 5 outputs for competition\n            nn.ReLU()\n        )\n\n        self._init_head_weights()\n    \n    def _init_head_weights(self):\n        \"\"\"Initialize regression head with deterministic weights\"\"\"\n        for m in self.regression_head.modules():\n            if isinstance(m, nn.Linear):\n                # Use a fixed seed for weight initialization\n                with torch.random.fork_rng():\n                    torch.manual_seed(SET_SEED)\n                    torch.nn.init.xavier_uniform_(m.weight)\n                    if m.bias is not None:\n                        torch.nn.init.zeros_(m.bias)\n    \n    def forward(self, x):\n        features = self.backbone(x)\n        return self.regression_head(features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T23:29:58.488104Z","iopub.execute_input":"2025-11-25T23:29:58.488360Z","iopub.status.idle":"2025-11-25T23:29:58.502960Z","shell.execute_reply.started":"2025-11-25T23:29:58.488311Z","shell.execute_reply":"2025-11-25T23:29:58.502230Z"}},"outputs":[],"execution_count":146},{"cell_type":"code","source":"base = '/kaggle/input'\ntrain_csv = f'{base}/csiro-biomass/train.csv'\ntest_csv = f'{base}/csiro-biomass/test.csv'\nextra_csv = f'{base}/grassclover-dataset/biomass_data/train/biomass_train_data.csv'\nextra_img = f'{base}/grassclover-dataset/biomass_data/train/images'\nbase_path = f'{base}/csiro-biomass/'\nsubmission_path = f'{base}/csiro-biomass/sample_submission.csv'\n\ndataset_df = pd.read_csv(train_csv)\ntest_df = pd.read_csv(test_csv)\nextra_df = pd.read_csv(extra_csv, sep=';')\nextra_img_path = extra_img\nunique_test_images = test_df['image_path'].unique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T23:29:58.503666Z","iopub.execute_input":"2025-11-25T23:29:58.503943Z","iopub.status.idle":"2025-11-25T23:29:58.529738Z","shell.execute_reply.started":"2025-11-25T23:29:58.503927Z","shell.execute_reply":"2025-11-25T23:29:58.529058Z"}},"outputs":[],"execution_count":147},{"cell_type":"code","source":"dataset_df['Sampling_Date'] = pd.to_datetime(dataset_df['Sampling_Date'], format='mixed')  # adjust format if needed\ndataset_df = dataset_df.pivot(\n    index=['image_path','Sampling_Date'],\n    columns='target_name',\n    values='target'\n).reset_index()\ndataset_df['Month'] = dataset_df['Sampling_Date'].dt.month","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T23:29:58.530576Z","iopub.execute_input":"2025-11-25T23:29:58.530827Z","iopub.status.idle":"2025-11-25T23:29:58.542191Z","shell.execute_reply.started":"2025-11-25T23:29:58.530806Z","shell.execute_reply":"2025-11-25T23:29:58.541622Z"}},"outputs":[],"execution_count":148},{"cell_type":"code","source":"def denormalize_targets(normalized_targets):\n    \"\"\"\n    Convert normalized targets back to original scale.\n    \n    Args:\n        normalized_targets: Tensor of shape [batch_size, 5] or [5] with normalized values\n    \n    Returns:\n        Tensor of same shape with denormalized values\n    \"\"\"\n    if normalized_targets.dim() == 1:\n        # Single sample: [5]\n        means = TARGET_MEANS.to(normalized_targets.device)\n        stds = TARGET_STDS.to(normalized_targets.device)\n        return normalized_targets * stds + means\n    else:\n        # Batch: [batch_size, 5]\n        means = TARGET_MEANS.to(normalized_targets.device).unsqueeze(0)  # [1, 5]\n        stds = TARGET_STDS.to(normalized_targets.device).unsqueeze(0)  # [1, 5]\n        return normalized_targets * stds + means\n\n\n# Normalization\ntarget_stats = {}\nfor col in TARGET_COLS:\n    target_stats[col] = {\n        'mean': dataset_df[col].mean(),\n        'std': dataset_df[col].std() + 1e-8\n    }\n    print(f\"{col}: mean={target_stats[col]['mean']:.2f}, std={target_stats[col]['std']:.2f}\")\n\n# Store for later denormalization\nTARGET_MEANS = torch.tensor([target_stats[col]['mean'] for col in TARGET_COLS], dtype=torch.float32)\nTARGET_STDS = torch.tensor([target_stats[col]['std'] for col in TARGET_COLS], dtype=torch.float32)\n\ndataset_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T23:29:58.544094Z","iopub.execute_input":"2025-11-25T23:29:58.544555Z","iopub.status.idle":"2025-11-25T23:29:58.560127Z","shell.execute_reply.started":"2025-11-25T23:29:58.544537Z","shell.execute_reply":"2025-11-25T23:29:58.559510Z"}},"outputs":[{"name":"stdout","text":"Dry_Clover_g: mean=6.65, std=12.12\nDry_Dead_g: mean=12.04, std=12.40\nDry_Green_g: mean=26.62, std=25.40\nDry_Total_g: mean=45.32, std=27.98\nGDM_g: mean=33.27, std=24.94\n","output_type":"stream"},{"execution_count":149,"output_type":"execute_result","data":{"text/plain":"target_name              image_path Sampling_Date  Dry_Clover_g  Dry_Dead_g  \\\n0            train/ID1011485656.jpg    2015-09-04        0.0000     31.9984   \n1            train/ID1012260530.jpg    2015-04-01        0.0000      0.0000   \n2            train/ID1025234388.jpg    2015-09-01        6.0500      0.0000   \n3            train/ID1028611175.jpg    2015-05-18        0.0000     30.9703   \n4            train/ID1035947949.jpg    2015-09-11        0.4343     23.2239   \n\ntarget_name  Dry_Green_g  Dry_Total_g    GDM_g  Month  \n0                16.2751      48.2735  16.2750      9  \n1                 7.6000       7.6000   7.6000      4  \n2                 0.0000       6.0500   6.0500      9  \n3                24.2376      55.2079  24.2376      5  \n4                10.5261      34.1844  10.9605      9  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>target_name</th>\n      <th>image_path</th>\n      <th>Sampling_Date</th>\n      <th>Dry_Clover_g</th>\n      <th>Dry_Dead_g</th>\n      <th>Dry_Green_g</th>\n      <th>Dry_Total_g</th>\n      <th>GDM_g</th>\n      <th>Month</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train/ID1011485656.jpg</td>\n      <td>2015-09-04</td>\n      <td>0.0000</td>\n      <td>31.9984</td>\n      <td>16.2751</td>\n      <td>48.2735</td>\n      <td>16.2750</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train/ID1012260530.jpg</td>\n      <td>2015-04-01</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>7.6000</td>\n      <td>7.6000</td>\n      <td>7.6000</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train/ID1025234388.jpg</td>\n      <td>2015-09-01</td>\n      <td>6.0500</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>6.0500</td>\n      <td>6.0500</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train/ID1028611175.jpg</td>\n      <td>2015-05-18</td>\n      <td>0.0000</td>\n      <td>30.9703</td>\n      <td>24.2376</td>\n      <td>55.2079</td>\n      <td>24.2376</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train/ID1035947949.jpg</td>\n      <td>2015-09-11</td>\n      <td>0.4343</td>\n      <td>23.2239</td>\n      <td>10.5261</td>\n      <td>34.1844</td>\n      <td>10.9605</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":149},{"cell_type":"markdown","source":"### Pytorch","metadata":{}},{"cell_type":"code","source":"train_transform = transforms.Compose([\n    transforms.Resize(IMAGE_SIZE),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomVerticalFlip(p=0.5),  # Grass can be flipped\n    transforms.RandomRotation(degrees=15),\n    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n    \n    # Color augmentations (grass varies in color)\n    transforms.ColorJitter(\n        brightness=0.1,  # Sunlight variations\n        contrast=0.1,     # Different lighting\n        saturation=0.1,  # Grass color variations\n        hue=0.1\n    ),\n    \n    # Advanced augmentations\n    transforms.RandomApply([\n        transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 0.5))\n    ], p=0.1),\n    # transforms.RandomHorizontalFlip(p=0.2),\n    # transforms.RandomVerticalFlip(p=0.1),\n    # transforms.RandomRotation(10),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                       std=[0.229, 0.224, 0.225])\n])\n\nval_transform = transforms.Compose([\n    transforms.Resize(IMAGE_SIZE),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                       std=[0.229, 0.224, 0.225])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T23:29:58.560833Z","iopub.execute_input":"2025-11-25T23:29:58.561073Z","iopub.status.idle":"2025-11-25T23:29:58.574845Z","shell.execute_reply.started":"2025-11-25T23:29:58.561054Z","shell.execute_reply":"2025-11-25T23:29:58.574155Z"}},"outputs":[],"execution_count":150},{"cell_type":"markdown","source":"### Support Functions","metadata":{}},{"cell_type":"code","source":"def forward_pass(images, targets, optimizer, model, validation=False):\n    images = images.to(device)\n    targets = targets.to(device)\n    \n    if not validation: \n        optimizer.zero_grad()\n    \n    outputs = model(images)\n    \n    loss = combined_biomass_loss(outputs, targets)\n    \n    if not validation:\n        loss.backward()\n        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n    \n    return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T23:29:58.575532Z","iopub.execute_input":"2025-11-25T23:29:58.575730Z","iopub.status.idle":"2025-11-25T23:29:58.590414Z","shell.execute_reply.started":"2025-11-25T23:29:58.575708Z","shell.execute_reply":"2025-11-25T23:29:58.589633Z"}},"outputs":[],"execution_count":151},{"cell_type":"markdown","source":"### Train Model","metadata":{"execution":{"iopub.status.busy":"2025-11-14T12:22:03.521564Z","iopub.status.idle":"2025-11-14T12:22:03.522083Z","shell.execute_reply":"2025-11-14T12:22:03.521846Z","shell.execute_reply.started":"2025-11-14T12:22:03.521827Z"}}},{"cell_type":"code","source":"def combined_biomass_loss(biomass_pred, biomass_true):\n    weights = torch.tensor(GIVEN_WEIGHTS, device=biomass_pred.device)\n\n    smooth_l1 = nn.SmoothL1Loss(reduction='none')\n    mse = nn.MSELoss(reduction='none')\n    \n    smooth_l1_loss = smooth_l1(biomass_pred, biomass_true)\n    mse_loss = mse(biomass_pred, biomass_true)\n    \n    combined = 0.3 * smooth_l1_loss + 0.7 * mse_loss\n    weighted_loss = (combined * weights).mean()\n    \n    return weighted_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T23:29:58.591106Z","iopub.execute_input":"2025-11-25T23:29:58.591708Z","iopub.status.idle":"2025-11-25T23:29:58.601824Z","shell.execute_reply.started":"2025-11-25T23:29:58.591692Z","shell.execute_reply":"2025-11-25T23:29:58.601020Z"}},"outputs":[],"execution_count":152},{"cell_type":"code","source":"def weighted_r2_score(sum_target, total_samples, sum_target_sq, ss_res):\n    mean_target = sum_target / total_samples\n    ss_tot = sum_target_sq - total_samples * (mean_target ** 2)\n\n    r2_per_output = 1 - ss_res / (ss_tot + 1e-10)\n\n    weights = torch.tensor(GIVEN_WEIGHTS, device=device)\n    r2_weighted = (r2_per_output * weights).sum() / weights.sum()\n    return r2_weighted","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T23:29:58.602477Z","iopub.execute_input":"2025-11-25T23:29:58.602650Z","iopub.status.idle":"2025-11-25T23:29:58.616570Z","shell.execute_reply.started":"2025-11-25T23:29:58.602637Z","shell.execute_reply":"2025-11-25T23:29:58.615859Z"}},"outputs":[],"execution_count":153},{"cell_type":"code","source":"def pretrain_phase(extra_df, extra_img_path, num_epochs=NUM_FT_EPOCHS):\n    print(\"=\" * 50)\n    print(\"PHASE 1: PRE-TRAINING ON EXTRA DATASET\")\n    \n    # Create dataset\n    extra_dataset = ExtraDataset(extra_df, extra_img_path, transform=train_transform)\n\n    generator = torch.Generator().manual_seed(SET_SEED)\n    \n    extra_train_size = int(0.8 * len(extra_dataset))\n    extra_dev_size = len(extra_dataset) - extra_train_size\n\n    extra_train_dataset, extra_dev_dataset = random_split(extra_dataset, [extra_train_size, extra_dev_size], generator=generator)\n\n    extra_loader = DataLoader(extra_train_dataset, batch_size=16, shuffle=TRAIN_SHUFFLE)\n    extra_dev_loader = DataLoader(extra_dev_dataset, batch_size=16, shuffle=False)  # usually no shuffle for dev\n    \n    # Initialize model\n    model = PreTrainModel().to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n    criterion = nn.SmoothL1Loss()\n    \n    gc.collect()\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n        for images, targets in extra_loader:\n            images, targets = images.to(device), targets.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(images).squeeze()\n            loss = criterion(outputs, targets)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            \n            total_loss += loss.item()\n            # validate_epoch(model, extra_dev_loader, criterion=criterion)\n        \n        print(f\"Pre-train Epoch {epoch+1}/{num_epochs} - Loss: {total_loss/len(extra_loader):.4f}\")\n    \n    return model.backbone ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T23:29:58.617255Z","iopub.execute_input":"2025-11-25T23:29:58.617470Z","iopub.status.idle":"2025-11-25T23:29:58.628305Z","shell.execute_reply.started":"2025-11-25T23:29:58.617445Z","shell.execute_reply":"2025-11-25T23:29:58.627679Z"}},"outputs":[],"execution_count":154},{"cell_type":"code","source":"def create_data_loaders(train_df, val_df, batch_size=BATCH_SIZE):\n    train_dataset = BiomassDataset(train_df, base_path, transform=train_transform)\n    val_dataset = BiomassDataset(val_df, base_path, transform=val_transform)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=TRAIN_SHUFFLE, num_workers=0)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n    return train_loader, val_loader\n\ndef train_epoch(model, train_loader, optimizer, quick):\n    model.train()\n    total_loss = 0\n    for i, (images, targets) in enumerate(train_loader):\n        if quick and i >= 1:\n            break\n            \n        loss = forward_pass(images, targets, optimizer, model)\n        total_loss += loss.item()\n    return total_loss / len(train_loader)\n\ndef validate_epoch(model, val_loader, criterion=None):\n    model.eval()\n    total_loss = 0\n    total_samples = 0\n\n    # Accumulate sums for R² calculation (no need to store all predictions)\n    ss_res = torch.zeros(5, device=device)\n    sum_target = torch.zeros(5, device=device)\n    sum_target_sq = torch.zeros(5, device=device)\n\n    with torch.no_grad():\n        for images, targets in val_loader:\n            images = images.to(device)\n            targets = targets.to(device)\n            \n            outputs = model(images)\n\n            if criterion:\n                loss = criterion(outputs, targets)\n            else:\n                loss = combined_biomass_loss(outputs, targets)\n            \n            total_loss += loss.item()\n            total_samples += outputs.shape[0] # batch_size\n            \n            outputs_denorm = denormalize_targets(outputs)\n            targets_denorm = denormalize_targets(targets)\n            \n            ss_res += ((outputs_denorm - targets_denorm) ** 2).sum(dim=0)\n            sum_target += targets_denorm.sum(dim=0)\n            sum_target_sq += (targets_denorm ** 2).sum(dim=0)\n\n    r2_weighted = weighted_r2_score(sum_target, total_samples, sum_target_sq, ss_res)\n\n    return total_loss / len(val_loader), r2_weighted\n\n\ndef compute_fold_metrics(epoch_train_losses, epoch_val_losses, epoch_val_r2s):\n    best_val_idx = np.argmin(epoch_val_losses)\n    best_val_loss = epoch_val_losses[best_val_idx]\n    best_val_r2 = epoch_val_r2s[best_val_idx]\n    overfit_metric = epoch_train_losses[best_val_idx] - best_val_loss\n    stability_val_loss = np.mean(epoch_val_losses[-5:])\n    stability_val_r2 = np.mean(epoch_val_r2s[-5:])\n\n    metrics = {\n        \"best_val_loss\": best_val_loss,\n        \"best_val_r2\": best_val_r2,\n        \"overfit_metric\": overfit_metric,\n        \"stability_val_loss\": stability_val_loss,\n        \"stability_val_r2\": stability_val_r2\n    }\n    return metrics\n\ndef print_fold_metrics(fold, metrics):\n    print(f\"\\n--- Fold {fold + 1} Metrics Summary ---\")\n    print(f\"Best Val Loss: {metrics['best_val_loss']:.4f}\")\n    print(f\"Best Val R²: {metrics['best_val_r2']:.4f}\")\n    print(f\"Overfit Metric (train - val at best epoch): {metrics['overfit_metric']:.4f}\")\n    print(f\"Average Val Loss (last 5 epochs): {metrics['stability_val_loss']:.4f}\")\n    print(f\"Average Val R² (last 5 epochs): {metrics['stability_val_r2']:.4f}\")\n\n\ndef finetune_phase(pretrained_backbone=None, num_epochs=NUM_FT_EPOCHS, quick=False):\n    print(\"\\n\" + \"=\" * 50)\n    print(\"PHASE 2: FINE-TUNING ON COMPETITION DATA\")\n    \n    r2 = []\n    fold_models = []\n\n    for fold, (train_idx, val_idx) in enumerate(splits):\n        print(f\"\\n--- Fold {fold + 1} Metrics Summary ---\")\n        train_df = dataset_df.iloc[train_idx].copy()\n        val_df = dataset_df.iloc[val_idx].copy()\n        train_loader, val_loader = create_data_loaders(train_df, val_df)\n\n        model = FinetuneModel().to(device)\n\n        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n\n        \n        epoch_train_losses = []\n        epoch_val_losses = []\n        epoch_val_r2s = []\n    \n\n        for epoch in range(num_epochs):\n            epoch_start = time.time()\n            train_loss = train_epoch(model, train_loader, optimizer, quick)\n            val_loss, val_r2 = validate_epoch(model, val_loader)\n\n            epoch_train_losses.append(train_loss)\n            epoch_val_losses.append(val_loss)\n            epoch_val_r2s.append(val_r2.item())\n            \n            print_result(train_loss, val_loss, epoch_start, epoch, num_epochs, val_r2)\n\n        r2.append(val_r2.item())\n        fold_models.append(model)\n        fold_metrics = compute_fold_metrics(epoch_train_losses, epoch_val_losses, epoch_val_r2s)\n        print_fold_metrics(fold, fold_metrics)\n\n    overall_r2 = np.array(r2).mean()\n    \n    print(f'\\nOverall R² across all folds: {overall_r2:.4f}')\n\n    return fold_models","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T23:29:58.628959Z","iopub.execute_input":"2025-11-25T23:29:58.629159Z","iopub.status.idle":"2025-11-25T23:29:58.645753Z","shell.execute_reply.started":"2025-11-25T23:29:58.629132Z","shell.execute_reply":"2025-11-25T23:29:58.645099Z"}},"outputs":[],"execution_count":155},{"cell_type":"code","source":"# pretrained_backbone = pretrain_phase(extra_df, extra_img_path, num_epochs=10)","metadata":{"execution":{"iopub.status.busy":"2025-11-25T23:29:58.646490Z","iopub.execute_input":"2025-11-25T23:29:58.646761Z","iopub.status.idle":"2025-11-25T23:29:58.659343Z","shell.execute_reply.started":"2025-11-25T23:29:58.646738Z","shell.execute_reply":"2025-11-25T23:29:58.658766Z"},"trusted":true,"_kg_hide-output":true},"outputs":[],"execution_count":156},{"cell_type":"code","source":"### NUM_FOLDS\n# NUM_FT_EPOCHS\n# kfold = GroupKFold(n_splits=NUM_FOLDS)\n# groups = dataset_df['Month']\n# splits = kfold.split(dataset_df, groups=groups)\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\n# Make a single random split\ntrain_idx, val_idx = train_test_split(\n    np.arange(len(dataset_df)),\n    test_size=0.2,\n    shuffle=False,\n    random_state=42\n)\n\n# Wrap it in a list so enumerate() still works\nsplits = [(train_idx, val_idx)]\nimport warnings\nwarnings.filterwarnings(\"ignore\", message=\".*does not have a deterministic implementation.*\")\nfinal_model = finetune_phase(pretrained_backbone=False,num_epochs=40, quick=False)\n# Epoch 1/40 - Train Loss: 0.1645, Val Loss: 0.1424 | R²: 0.0942 | Time: 0m 40s\n# Epoch 2/40 - Train Loss: 0.1284, Val Loss: 0.1148 | R²: 0.2658 | Time: 0m 27s\n# Epoch 3/40 - Train Loss: 0.1011, Val Loss: 0.1003 | R²: 0.3627 | Time: 0m 27s\n# Epoch 4/40 - Train Loss: 0.0802, Val Loss: 0.1005 | R²: 0.3636 | Time: 0m 27s\n# Epoch 5/40 - Train Loss: 0.0816, Val Loss: 0.0999 | R²: 0.3692 | Time: 0m 27s\n# Epoch 6/40 - Train Loss: 0.0742, Val Loss: 0.0894 | R²: 0.4357 | Time: 0m 27s\n# Epoch 7/40 - Train Loss: 0.0757, Val Loss: 0.0892 | R²: 0.4407 | Time: 0m 27s\n# Epoch 8/40 - Train Loss: 0.0713, Val Loss: 0.0929 | R²: 0.4147 | Time: 0m 27s\n# Epoch 9/40 - Train Loss: 0.0741, Val Loss: 0.0890 | R²: 0.4445 | Time: 0m 27s\n# Epoch 10/40 - Train Loss: 0.0657, Val Loss: 0.0834 | R²: 0.4740 | Time: 0m 27s\n# Epoch 11/40 - Train Loss: 0.0662, Val Loss: 0.0873 | R²: 0.4419 | Time: 0m 27s\n# Epoch 12/40 - Train Loss: 0.0577, Val Loss: 0.0813 | R²: 0.4830 | Time: 0m 27s\n# Epoch 13/40 - Train Loss: 0.0529, Val Loss: 0.0846 | R²: 0.4671 | Time: 0m 27s\n# Epoch 15/40 - Train Loss: 0.0508, Val Loss: 0.0844 | R²: 0.4676 | Time: 0m 28s\n# Epoch 16/40 - Train Loss: 0.0593, Val Loss: 0.0844 | R²: 0.4698 | Time: 0m 27s\n# Epoch 17/40 - Train Loss: 0.0553, Val Loss: 0.0799 | R²: 0.5002 | Time: 0m 28s\n# Epoch 18/40 - Train Loss: 0.0503, Val Loss: 0.0881 | R²: 0.4480 | Time: 0m 28s\n# Epoch 19/40 - Train Loss: 0.0533, Val Loss: 0.0807 | R²: 0.4952 | Time: 0m 28s\n# --- efficientb \n# Epoch 1/40 - Train Loss: 0.1669, Val Loss: 0.1252 | R²: 0.2158 | Time: 0m 50s\n# Epoch 2/40 - Train Loss: 0.1363, Val Loss: 0.1012 | R²: 0.3694 | Time: 0m 50s\n# Epoch 3/40 - Train Loss: 0.1186, Val Loss: 0.0877 | R²: 0.4542 | Time: 0m 50s\n# Epoch 4/40 - Train Loss: 0.1043, Val Loss: 0.0841 | R²: 0.4783 | Time: 0m 50s\n# Epoch 5/40 - Train Loss: 0.0916, Val Loss: 0.0814 | R²: 0.4980 | Time: 0m 50s\n# Epoch 6/40 - Train Loss: 0.0825, Val Loss: 0.0801 | R²: 0.5055 | Time: 0m 49s\n# Epoch 7/40 - Train Loss: 0.0831, Val Loss: 0.0750 | R²: 0.5377 | Time: 0m 50s\n# Epoch 8/40 - Train Loss: 0.0733, Val Loss: 0.0757 | R²: 0.5345 | Time: 0m 50s\n# Epoch 9/40 - Train Loss: 0.0717, Val Loss: 0.0778 | R²: 0.5194 | Time: 0m 50s\n# Epoch 10/40 - Train Loss: 0.0693, Val Loss: 0.0745 | R²: 0.5398 | Time: 0m 50s\n# Epoch 11/40 - Train Loss: 0.0671, Val Loss: 0.0713 | R²: 0.5599 | Time: 0m 50s\n# Epoch 12/40 - Train Loss: 0.0630, Val Loss: 0.0724 | R²: 0.5511 | Time: 0m 50s\n# Epoch 13/40 - Train Loss: 0.0587, Val Loss: 0.0759 | R²: 0.5284 | Time: 0m 50s\n# Epoch 14/40 - Train Loss: 0.0631, Val Loss: 0.0732 | R²: 0.5463 | Time: 0m 50s\n# Epoch 15/40 - Train Loss: 0.0568, Val Loss: 0.0688 | R²: 0.5731 | Time: 0m 50s\n# Epoch 16/40 - Train Loss: 0.0526, Val Loss: 0.0679 | R²: 0.5778 | Time: 0m 50s\n# Epoch 17/40 - Train Loss: 0.0543, Val Loss: 0.0669 | R²: 0.5858 | Time: 0m 50s\n# --- With  with higher weight decay\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T23:29:58.660165Z","iopub.execute_input":"2025-11-25T23:29:58.660920Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nPHASE 2: FINE-TUNING ON COMPETITION DATA\n\n--- Fold 1 Metrics Summary ---\nEpoch 1/40 - Train Loss: 0.1575, Val Loss: 0.1277 | R²: 0.1926 | Time: 2m 36s\nEpoch 2/40 - Train Loss: 0.1381, Val Loss: 0.1101 | R²: 0.3069 | Time: 2m 37s\nEpoch 3/40 - Train Loss: 0.1231, Val Loss: 0.1014 | R²: 0.3653 | Time: 2m 41s\nEpoch 4/40 - Train Loss: 0.1102, Val Loss: 0.0857 | R²: 0.4698 | Time: 2m 50s\nEpoch 5/40 - Train Loss: 0.1034, Val Loss: 0.0860 | R²: 0.4670 | Time: 2m 44s\nEpoch 6/40 - Train Loss: 0.0946, Val Loss: 0.0734 | R²: 0.5480 | Time: 2m 39s\nEpoch 7/40 - Train Loss: 0.0855, Val Loss: 0.0762 | R²: 0.5293 | Time: 2m 40s\nEpoch 8/40 - Train Loss: 0.0762, Val Loss: 0.0829 | R²: 0.4856 | Time: 2m 34s\nEpoch 9/40 - Train Loss: 0.0694, Val Loss: 0.0803 | R²: 0.5006 | Time: 2m 32s\nEpoch 10/40 - Train Loss: 0.0639, Val Loss: 0.0788 | R²: 0.5108 | Time: 2m 35s\nEpoch 11/40 - Train Loss: 0.0619, Val Loss: 0.0800 | R²: 0.5032 | Time: 2m 34s\nEpoch 12/40 - Train Loss: 0.0598, Val Loss: 0.0773 | R²: 0.5135 | Time: 2m 36s\nEpoch 13/40 - Train Loss: 0.0556, Val Loss: 0.0949 | R²: 0.3967 | Time: 2m 38s\nEpoch 14/40 - Train Loss: 0.0529, Val Loss: 0.0817 | R²: 0.4806 | Time: 2m 36s\nEpoch 15/40 - Train Loss: 0.0519, Val Loss: 0.0881 | R²: 0.4380 | Time: 2m 34s\nEpoch 16/40 - Train Loss: 0.0516, Val Loss: 0.0836 | R²: 0.4681 | Time: 2m 34s\nEpoch 17/40 - Train Loss: 0.0491, Val Loss: 0.0755 | R²: 0.5213 | Time: 2m 36s\nEpoch 18/40 - Train Loss: 0.0494, Val Loss: 0.0833 | R²: 0.4694 | Time: 2m 34s\nEpoch 19/40 - Train Loss: 0.0447, Val Loss: 0.0666 | R²: 0.5776 | Time: 2m 34s\nEpoch 20/40 - Train Loss: 0.0442, Val Loss: 0.0714 | R²: 0.5490 | Time: 2m 34s\nEpoch 21/40 - Train Loss: 0.0404, Val Loss: 0.0708 | R²: 0.5518 | Time: 2m 34s\nEpoch 22/40 - Train Loss: 0.0399, Val Loss: 0.0742 | R²: 0.5289 | Time: 2m 34s\nEpoch 23/40 - Train Loss: 0.0388, Val Loss: 0.0541 | R²: 0.6612 | Time: 2m 34s\nEpoch 24/40 - Train Loss: 0.0401, Val Loss: 0.0717 | R²: 0.5479 | Time: 2m 33s\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"test_dataset = BiomassDataset(test_df, base_path, transform=val_transform)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\nfold_models = final_model\nensemble_outputs = []\nwith torch.no_grad():\n    for image, img_path in test_loader:\n        image = image.to(device)\n        \n        # Get predictions from all models\n        all_outputs = []\n        for model in fold_models:\n            model.eval()\n            outputs = model(image)\n            print(outputs)\n            all_outputs.append(outputs)\n            ensemble_outputs.append(torch.stack(all_outputs).mean(dim=0))\n        \nensemble_outputs = torch.cat(ensemble_outputs, dim=0)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset = BiomassDataset(test_df, base_path, transform=val_transform)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n\n# Dictionary to store predictions grouped by image_path\npreds_by_image = defaultdict(list)\n\nwith torch.no_grad():\n    for image, img_paths in test_loader:\n        image = image.to(device)\n        \n        # Get ensemble predictions from all models\n        all_outputs = []\n        for model in fold_models:\n            model.eval()\n            outputs = model(image)\n            all_outputs.append(outputs)\n        \n        # Average across models (ensemble)\n        ensemble_batch = torch.stack(all_outputs).mean(dim=0)  # [batch_size, 5]\n        \n        # Group predictions by image_path\n        # Since BiomassDataset returns 2 items per image (left/right halves),\n        # img_paths is a tuple/list of image paths (may have duplicates)\n        # We group by image_path and will average left/right later\n        batch_size = ensemble_batch.shape[0]\n        for i in range(batch_size):\n            img_path = img_paths[i]  # Get the image_path for this batch item\n            preds_by_image[img_path].append(ensemble_batch[i].cpu())\n\n# Average left/right predictions for each image\nimage_predictions = {}\nfor img_path, preds in preds_by_image.items():\n    # Stack and average: [num_halves, 5] -> [5]\n    # Each image should have exactly 2 predictions (left + right)\n    stacked_preds = torch.stack(preds)  # [2, 5] for left and right halves\n    image_predictions[img_path] = stacked_preds.mean(dim=0)  # Average to [5]ee\n\n# Check predictions for first image\nif len(image_predictions) > 0:\n    first_img_path = list(image_predictions.keys())[0]\n    print(f\"Image: {first_img_path}\")\n    print(f\"Predictions: {image_predictions[first_img_path].tolist()}\")\n    print(f\"Target columns: {TARGET_COLS}\")\nelse:\n    print(\"No predictions available yet. Run the test prediction cell first.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Denormalize all predictions back to original scale\nfor img_path in image_predictions:\n    image_predictions[img_path] = denormalize_targets(image_predictions[img_path])\n\nprint(f\"Denormalized predictions for {len(image_predictions)} images\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ensemble_outputs[0].tolist()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_df = pd.read_csv(submission_path)\n\nfor i, row in submission_df.iterrows():\n    img_path = test_df[test_df['sample_id'] == row['sample_id']]['image_path'].values[0]\n    target_name = test_df[test_df['sample_id'] == row['sample_id']]['target_name'].values[0]\n    \n    preds = ensemble_outputs\n    target_idx = TARGET_COLS.index(target_name)\n    submission_df.at[i, 'target'] = ensemble_outputs[i, target_idx].item()\n\nsubmission_df.to_csv('submission.csv', index=False)\nsubmission_df","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}